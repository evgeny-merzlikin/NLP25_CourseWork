import os
import uuid
import json
import datetime
from pathlib import Path
from typing import Optional

from pydantic import SecretStr
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import HumanMessage
from src.logger import logger

HISTORY_DIR = Path("data/history")
HISTORY_DIR.mkdir(parents=True, exist_ok=True)


class InsuranceAnalyzer:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ —É—Å–ª–æ–≤–∏–π —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API.
    """

    def __init__(
        self,
        model_name: str = "google/gemini-2.5-flash-pre-05-20",
        temperature: float = 0.1,
        base_url: str = "https://api.vsegpt.ru/v1"
    ):
        api_key_raw = os.getenv("OPENAI_API_KEY")
        if not api_key_raw:
            raise EnvironmentError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

        self.api_key: SecretStr = SecretStr(api_key_raw)

        self.llm = ChatOpenAI(
            model=model_name,
            temperature=temperature,
            api_key=self.api_key,
            base_url=base_url,
        )

        logger.info(f"InsuranceAnalyzer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –º–æ–¥–µ–ª—å—é: {model_name}")

    def analyze(
        self,
        contract_text: str,
        rules_text: str,
        template_text: str
    ) -> dict:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞ –∏ –ø—Ä–∞–≤–∏–ª —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏ –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π.
        """
        try:
            prompt = PromptTemplate(
                input_variables=["contract_text", "rules_text"],
                template=template_text
            )
            prompt_text = prompt.format(
                contract_text=contract_text,
                rules_text=rules_text
            )

            logger.info("üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ LLM...")
            messages = [HumanMessage(content=prompt_text)]
            response = self.llm.invoke(messages)
            response_text = response.content

            logger.info("‚úÖ –û—Ç–≤–µ—Ç LLM –ø–æ–ª—É—á–µ–Ω")

            result = {
                "id": str(uuid.uuid4()),
                "timestamp": datetime.datetime.now().isoformat(),
                "template": template_text,
                "prompt_text": prompt_text,
                "contract_chars": len(contract_text),
                "rules_chars": len(rules_text),
                "prompt_chars": len(prompt_text),
                "response": response_text,
                "tokens": getattr(response, "usage", None)
            }

            output_path = HISTORY_DIR / f"{result['id']}.json"
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(result, f, indent=2, ensure_ascii=False)

            logger.info(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path.name}")

            return result

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–Ω–∞–ª–∏–∑–µ: {e}", exc_info=True)
            raise RuntimeError("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–æ–≥–æ–≤–æ—Ä–∞ –∏ –ø—Ä–∞–≤–∏–ª") from e
